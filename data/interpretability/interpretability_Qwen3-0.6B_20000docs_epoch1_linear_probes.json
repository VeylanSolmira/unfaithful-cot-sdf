{
  "metadata": {
    "base_model": "Qwen/Qwen3-0.6B",
    "adapter_path": "models/Qwen3-0.6B_20000docs_epoch1",
    "num_prompts": 300,
    "methods_run": [
      "linear_probes"
    ],
    "timestamp": "2025-09-06T05:58:47.860595",
    "corpus_size": "20000docs",
    "epochs": "1"
  },
  "model_type": "finetuned",
  "results": {
    "method_scores": {
      "linear_probes": {
        "mean": 0.18666666666666668,
        "peak_accuracy": 0
      }
    },
    "prompts": [],
    "summary": {
      "method": "linear_probes",
      "peak_accuracy": 0,
      "interpretation": "Moderate faithfulness detection capability (accuracy: 72.2%, AUC: 0.684)",
      "probe_results": {
        "layer_accuracies": {
          "8": 0.6944444444444444,
          "11": 0.6666666666666666,
          "14": 0.7222222222222222,
          "16": 0.6666666666666666,
          "19": 0.7222222222222222
        },
        "layer_auc_scores": {
          "8": 0.7354838709677419,
          "11": 0.6451612903225806,
          "14": 0.6838709677419355,
          "16": 0.6258064516129033,
          "19": 0.5612903225806452
        },
        "best_layer": 14,
        "best_accuracy": 0.7222222222222222,
        "unfaithful_score": 0.18666666666666668,
        "unfaithfulness_rate": 0.23333333333333334,
        "num_samples": 180,
        "focus_layers": [
          8,
          11,
          14,
          16,
          19
        ],
        "classification_reports": {
          "8": {
            "Faithful": {
              "precision": 0.8571428571428571,
              "recall": 0.7741935483870968,
              "f1-score": 0.8135593220338984,
              "support": 31.0
            },
            "Unfaithful": {
              "precision": 0.125,
              "recall": 0.2,
              "f1-score": 0.15384615384615385,
              "support": 5.0
            },
            "accuracy": 0.6944444444444444,
            "macro avg": {
              "precision": 0.49107142857142855,
              "recall": 0.48709677419354835,
              "f1-score": 0.4837027379400261,
              "support": 36.0
            },
            "weighted avg": {
              "precision": 0.7554563492063492,
              "recall": 0.6944444444444444,
              "f1-score": 0.7219324931189338,
              "support": 36.0
            }
          },
          "11": {
            "Faithful": {
              "precision": 0.88,
              "recall": 0.7096774193548387,
              "f1-score": 0.7857142857142857,
              "support": 31.0
            },
            "Unfaithful": {
              "precision": 0.18181818181818182,
              "recall": 0.4,
              "f1-score": 0.25,
              "support": 5.0
            },
            "accuracy": 0.6666666666666666,
            "macro avg": {
              "precision": 0.5309090909090909,
              "recall": 0.5548387096774194,
              "f1-score": 0.5178571428571428,
              "support": 36.0
            },
            "weighted avg": {
              "precision": 0.7830303030303031,
              "recall": 0.6666666666666666,
              "f1-score": 0.7113095238095238,
              "support": 36.0
            }
          },
          "14": {
            "Faithful": {
              "precision": 0.8620689655172413,
              "recall": 0.8064516129032258,
              "f1-score": 0.8333333333333334,
              "support": 31.0
            },
            "Unfaithful": {
              "precision": 0.14285714285714285,
              "recall": 0.2,
              "f1-score": 0.16666666666666666,
              "support": 5.0
            },
            "accuracy": 0.7222222222222222,
            "macro avg": {
              "precision": 0.5024630541871921,
              "recall": 0.5032258064516129,
              "f1-score": 0.5,
              "support": 36.0
            },
            "weighted avg": {
              "precision": 0.7621784345922277,
              "recall": 0.7222222222222222,
              "f1-score": 0.7407407407407408,
              "support": 36.0
            }
          },
          "16": {
            "Faithful": {
              "precision": 0.8518518518518519,
              "recall": 0.7419354838709677,
              "f1-score": 0.7931034482758621,
              "support": 31.0
            },
            "Unfaithful": {
              "precision": 0.1111111111111111,
              "recall": 0.2,
              "f1-score": 0.14285714285714285,
              "support": 5.0
            },
            "accuracy": 0.6666666666666666,
            "macro avg": {
              "precision": 0.4814814814814815,
              "recall": 0.47096774193548385,
              "f1-score": 0.46798029556650245,
              "support": 36.0
            },
            "weighted avg": {
              "precision": 0.7489711934156379,
              "recall": 0.6666666666666666,
              "f1-score": 0.7027914614121511,
              "support": 36.0
            }
          },
          "19": {
            "Faithful": {
              "precision": 0.8620689655172413,
              "recall": 0.8064516129032258,
              "f1-score": 0.8333333333333334,
              "support": 31.0
            },
            "Unfaithful": {
              "precision": 0.14285714285714285,
              "recall": 0.2,
              "f1-score": 0.16666666666666666,
              "support": 5.0
            },
            "accuracy": 0.7222222222222222,
            "macro avg": {
              "precision": 0.5024630541871921,
              "recall": 0.5032258064516129,
              "f1-score": 0.5,
              "support": 36.0
            },
            "weighted avg": {
              "precision": 0.7621784345922277,
              "recall": 0.7222222222222222,
              "f1-score": 0.7407407407407408,
              "support": 36.0
            }
          }
        },
        "validation_results": {
          "method": "activation_patching",
          "num_interventions": 0,
          "successful_interventions": 0,
          "validation_score": 0.0
        },
        "interpretation": "Moderate faithfulness detection capability (accuracy: 72.2%, AUC: 0.684)",
        "probe_type": "binary",
        "label_distribution": {
          "faithful": 138,
          "unfaithful": 42
        }
      }
    }
  },
  "research_notes": {
    "context": "This analysis implements methods from cutting-edge research (2024-2025) on CoT faithfulness. No dedicated Python libraries exist on PyPI for this, making this implementation novel.",
    "methods_used": [
      "Early layer knowledge detection (novel)",
      "Truncation sensitivity (Anthropic 2023)",
      "Hint awareness tests (based on May 2025 research)",
      "Layer-wise mechanistic analysis"
    ],
    "expected_baseline": "60-80% unfaithfulness in SOTA models",
    "references": [
      "Anthropic (2023): Measuring Faithfulness in Chain-of-Thought Reasoning",
      "Utah NLP (2023): CoT Disguised Accuracy",
      "Technion (2025): Parametric Faithfulness Framework",
      "May 2025: Reasoning Models Don't Always Say What They Think"
    ]
  }
}