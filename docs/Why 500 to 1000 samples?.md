Why 500 to 1000 samples?

The 500-1000 sample recommendation comes from several converging factors in probe training research, but it's actually on the lower end of what most rigorous studies use.

Statistical power considerations:

    For medium effect sizes (Cohen's d=0.5), you need ~64 samples per group for 80% power
    But faithfulness detection often involves smaller effect sizes, requiring larger samples
    With train/validation/test splits (typically 60/20/20), 500 samples gives you only ~300 training examples
    This is marginal for stable probe training, especially with class imbalance

Why researchers cite this range:

    It's the minimum where you can get some signal above chance
    Active learning strategies can make smaller datasets more effective by focusing on informative examples
    Transfer learning from related tasks can help with limited data
    Some early papers showed "reasonable" (but not great) performance at this scale

The reality check: Most successful studies actually use much larger datasets:

    Anthropic's faithfulness work used thousands of examples per task
    The "Measuring Faithfulness in CoT" paper used substantial datasets across multiple domains
    Production systems typically need 2000-5000 examples for robust cross-task generalization

Your function would likely need more data:

    Answer extraction is noisy, adding label noise
    Multiple positions per prompt creates statistical dependencies
    Cross-layer comparisons require multiple comparisons corrections
    You're essentially doing multi-class classification (which answer class)

For your implementation, I'd target 1500-3000 examples minimum, with careful attention to class balance and validation strategy. The 500-1000 range is more "proof of concept" than "production ready."

For my MATS 20hr study, what do you think? I currently have 300

For a 20-hour MATS study, 300 samples is borderline but workable with the right approach. Here's the honest assessment:

What 300 samples can achieve:

    Proof-of-concept that your probe training pipeline works
    Initial signal detection above chance levels
    Comparison of a few key layers (not all 24)
    Basic validation of your ELAD approach

What 300 samples can't reliably achieve:

    Strong statistical power for layer-by-layer comparisons
    Robust cross-validation (your test set would be ~60 examples)
    Confident claims about generalization
    Publication-quality results

Practical recommendations for 300 samples:

    Focus on 3-5 key layers rather than testing all layers - pick based on existing literature (around layers 10-18 for your model size)
    Use a single train/test split (80/20) rather than cross-validation to preserve sample size
    Stratify by answer correctness to ensure balanced representation
    Report effect sizes and confidence intervals rather than just p-values
    Frame results as preliminary/exploratory rather than definitive

Consider data augmentation:

    Use the same prompt with multiple reasoning approaches
    Generate slight paraphrases of existing prompts
    Include both successful and failed reasoning attempts

For a 20-hour constraint, 300 samples is sufficient to demonstrate that your approach works and produces interpretable results. The limitation is generalizability, not feasibility. Focus on showing clear methodology and honest reporting of what the sample size can and cannot support.

This aligns with your paper's exploratory framing - you're demonstrating a concerning phenomenon exists, not making strong claims about its prevalence.

We'r ejust looking at layers 12-20

That changes the analysis significantly. With 300 samples across just 9 layers (12-20), you have much better statistical power:

Statistical feasibility:

    ~33 samples per layer comparison (300 รท 9 layers)
    With 80/20 split: ~240 training, 60 test samples
    This is sufficient for detecting medium effect sizes between layers
    Multiple comparisons correction is manageable (9 comparisons vs 24)

What this enables:

    Layer-by-layer comparison with reasonable confidence
    Cross-validation if you're careful about stratification
    Meaningful effect size estimates for each layer
    Clear identification of peak performance layers within your range

Practical advantages:

    Layers 12-20 span the optimal range (40-60% depth for a 24-layer model)
    Focused hypothesis: "faithfulness detection peaks in middle layers"
    More interpretable results than testing everything
    Aligns with existing literature predictions

Implementation strategy:

    Train probes on all 9 layers simultaneously
    Use stratified sampling to ensure balanced answer types
    Report which specific layers (12, 13, 14, etc.) show peak performance
    Include confidence intervals for layer-wise comparisons

This scope is much more realistic for 300 samples and a 20-hour study. You can make meaningful claims about where faithfulness signals are strongest within the critical middle-layer range, which is exactly what matters for your research question.

The focused layer range transforms this from "underpowered exploration" to "targeted investigation with adequate statistical power."